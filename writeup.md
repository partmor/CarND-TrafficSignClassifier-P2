# **Traffic Sign Recognition** 

## Writeup
---


[//]: # (Image References)

[histograms]: ./examples/histograms.png
[set30]: ./examples/set30.png
[original_sample]: ./examples/original_sample.png 
[prep_sample]: ./examples/prep_sample.png 
[architecture]: ./examples/architecture.jpg
[prep_sample_2]: ./examples/prep_sample_2.png 
[deformed_sample]: ./examples/deformed_sample.png 
[f1]: ./examples/test1_fullsize.jpg 
[f2]: ./examples/test2_fullsize.jpg 
[f3]: ./examples/test3_fullsize.jpg 
[f4]: ./examples/test4_fullsize.jpg 
[f5]: ./examples/test5_fullsize.jpg 
[f7]: ./examples/test7_fullsize.jpg 
[cn1_filters]: ./examples/cn1_filters.png 
[cn1_fmaps]: ./examples/cn1_fmaps.png 
[new_test_cases]: ./examples/new_test_cases.png 

### Data Set Summary & Exploration

The [given German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) consists of a total of 51839 32x32 px color images (3-channel RGB), already split into the training, validation, and test sets:

| Data set			    |     Number of samples | 
|:---------------------:|:---------------------:| 
| Training      		| 34799					| 
| Validation     		| 4410 					|
| Test					| 12630					|

The whole dataset contains samples of 43 types of traffic signs, listed [here](signnames.csv).

![histograms]

Certain classes are much more abundant than others: the datasets are highly unbalanced. However, the given train, validation and test sets present a similar composition regarding the proportion of samples for each class. The latter is an important fact to bear in mind when assessing the performance of a classifier in a scenario that inherently involves rare classes: validation/test sets should preserve the percentage of samples for each class for the validation/test scores to be meaningful. Indeed, the given sets seem to have been split in a stratified manner.

It is also worth highlighting that the images in the dataset have been extracted from video-tracks. Hence, groups of samples represent the same "real world instance", but captured under different conditions (distance, angle, speed, etc.) - corresponding to a changing position of the vehicle with respect to the sign: 

![set30]

This is a reminder that sample shuffling is crucial during training. 

A final fact to note is that lighting conditions and *integrity* of the images vary hugely throughout the dataset: images can be dark, blurry due to vehicle movement, or partially ocluded by surrounding objects.

###  Data preprocessing

The preprocessing pipeline of the data consists of the following steps;
+ Conversion to **grayscale**: [Sermanet et al.](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf) suggest that models using luminance solely (Y channel) outperform those using RGB channels.
+ [0, 1] **normalization**: highly convenient for a quick convergence of the optimization algorithm in the training phase.
+ Per-sample **mean substracion**: this normalization has the property of removing the average brightness (intensity) of the data point. In this case, we are not interested in the illumination conditions of the image, but more so in the content.

For instance, the following samples in the training set:

![original_sample]

are transformed into:

![prep_sample]

#### Training data augmentation

To help the model generalize better, the training dataset is augmented by *perturbing* the given samples. ConvNets architectures have built-in invariance to small translations, scaling and rotations. When a dataset does not naturally contain those deformations, adding them synthetically will yield more robust learning to potential deformations in the test set.

In this project, the augmented data is generated by applying a generic [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) to each training sample, yielding a training set 2x bigger than the original, with the same class composition.

An affinity is applied since it can account jointly for the following effects:
+ Rotation
+ Translation (image shifts)
+ Shear

In summary, each sample is transformed via affinity with random values drawn from a truncated normal distribution for rotation (20 degrees max.), shear (20 degrees max.), and translation (2px max.). 

A set of preprocessed images like the following:

![prep_sample_2]

and their affine transforms:

![deformed_sample]

A potential issue that has not been covered here is the class unbalance. The model's robustness could further be improved by generating *perturbed* samples at a rate such that the class distribution within the training is uniform.

Further transformations that could be applied to the data with the aim to improve robustness by simulating other real-world effects are Gaussian kernels (this would simulate movement blur), or random noise (faulty receptors). 

### Model Architecture

For this challenge I use a **2-stage ConvNet** with **multi-scale** feature architecture (MS) followed by a classifier that consists of **two fully connected layers**. 

![architecture]

Usual ConvNets are organized in strict feed-forward layered architectures in which the output of one layer is fed only to the layer above (single-scale feature architecture). Instead, in a multi-scale architecture, the output of the first stage is branched out and fed to the classifier, in addition to the output of the second stage. Additionally, a second subsampling stage is applied on the branched output, yielding higher accuracies than with just one. 

The motivation for combining representation from multiple stages in the classifier is to provide different scales of receptive fields to the classifier. The second stage extracts global and invariant shapes and structures, while the first stage extracts local motifs with more precise details.

A summary of the parameters used for the model can be found in the following table:

| Stage    |     Layer | Description | Output size |
|:---------------------:|:---------------------:| :---------------------:|:---------------------:|
| **Input**      		| 	Input		| grayscaled image (Y channel only) | 32x32x1|
| **1st stage ConvNet (CN1)**      		| 	Convolution		| 30 5x5 filters; 1x1 stride; valid padding | 28x28x30 |
|       		|  	ReLU		| | 28x28x30
| 				| Max Pooling	|2x2 stride; valid padding| 14x14x30
| **2nd stage ConvNet (CN2)**      		| 	Convolution		| 60 5x5 filters; 1x1 stride; valid padding | 10x10x60 |
|       		|  	ReLU		| | 10x10x60 |
| 				| Max Pooling	|2x2 stride; valid padding| 5x5x60 |
| **CN1 branch**      		| 	Max Pooling		| Pooling applied on CN1 output; 4x4 stride; valid padding | 3x3x30
| **Preparation for classifier**		|  	Flattening and concatenation		| Branched CN1 output and CN2 output are flattened and concatenated | 1x1770
| **1st stage FC (FC1)**				| Fully connected layer	|100 hidden units| 1x100
| 	| ReLU	| | 1x100
| **2nd stage FC (FC2)**				| Fully connected layer	|100 hidden units| 1x100
| 	| ReLU	| | 1x100
| **Output**				| Fully connected layer	|43 units| 1x43
| 	| Softmax	| | 1x43

### Model training

The objective loss function to minimize is the **cross entropy** over the training set. This function is minimized using the **Adam optimizer**, a method that computes adaptive learning rates for each parameter. [Kingma et al., 2014](https://arxiv.org/abs/1412.6980), show empirically that Adam compares favorably to other adaptive learning-method algorithms. 

During the training process, training and validation accuracy scores are monitored to ensure overfitting (continue training after a stagnation of validation performance) is avoided.

Overfitting is mitigated via:
+ **Dropout**: applied after the activations of CN1, CN2, FC1 and FC2, with ascending values towards the output, as suggested by [Srivastava et al., 2014](https://www.google.es/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwi-p5Ci34vTAhUEPBQKHUnMDegQFggcMAA&url=https%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fabsps%2FJMLRdropout.pdf&usg=AFQjCNFModVeeXkqtxn_TXeKPB0zFtw5ew&sig2=NAInnEhv1iJyKfk5yx87Sw). More aggressive dropout on the fully connected layers have proven to work better: [10, 20, 50, 50] % respectively.
+ **L2 regularization**:  applied only on weights of the fully connected layers. A scale of 1e-4 yielded best performance.

In order to reduce the number of explicit hyperparameters for this problem, CN and FC weights are initialized using the **Xavier initializer**. This initializer (Glorot et al., 2010) is designed to keep the scale of the gradients roughly the same in all layers.

Using a learning rate of 5e-4, a batch size of 128, and 50 epochs with the given hyperparameter setup yielded a *winning* model with the following performance metrics:

| Dataset			    |     Accuracy | 
|:---------------------:|:---------------------:| 
| Training      		| 0.986				| 
| Validation     		| 0.978 					|
| Test					| 0.958				|

The multi-scale architecture was chosen since it outperforms single-scale architectures like LeNet-5 for this particular problem. From my experience with this project, tweaking the modified Lenet-5 (and similar architectures) could hardly take the model beyond the 0.93 on validation set and 0.91 on test set; using the MS model, this could be accomplished after less than 10 epochs, with a lower training accuracy, i.e, with a lower propensity to overfitting.

### Testing Model on New Images

Here are some examples of German traffic signs I found on the web (original fullsize). Some of them were chosen for a particular reason in order to challenge the model:

![f1] 

From the latter image I pull two cases: one with a tight bounding box to the 30 km/h limit sign (easy), and another with a wither box that also includes the text sign underneath (more difficult, has interaction with another unknown sign). 

![f2] ![f3]

![f4] 

The above sample is interesting since it is partially ocluded by snow.

![f5] 

From this last image I generate two samples: one with a tight box on the yield sign (easy), and another including the other sign partially to add difficulty.

![f7] 

The custom test set finally consists of the following data:

![new_test_cases]

| Test sample			    |     Class | 
|:---------------------:|:---------------------:| 
| 1      		| 1	- Speed limit (30km/h)		| 
| 2     		| 14 -Stop				|
| 3					|28	- Children crossing			|
| 4      		| 30 - Beware of ice/snow			| 
| 5     		| 13 - Yield					|
| 6					|13 - Yield				|
| 7     		| 12 - Priority road					|
| 8					|1 - Speed limit (30km/h)				|

The accuracy for this mini-test set is 0.750 (correctly classified 6/8 cases), noticeably lower than the score obtained on the given test set. Two facts should be highlighted:
+ This set is not big enough for a population-averaged score to be meaningful at all (noise has a huge impact on outcome).
+ This set has been intentionally tweaked to contain singularities to challenge the model. 

### Neural Network Visualization

As an illustration of what a trained neural network looks like, I present a visualization of the first stage ConvNet filter weights, and another visualization for the 1st stage ConvNet feature maps of a sample fed to the model (that of test case 1).

The 1st stage ConvNet filter weights can be represented as 32 5x5 px grayscale images (to improve visualization a bilinear interpolation has been applied):

![cn1_filters]

As expected the first layer contains filters that can detect very local patterns, like edges and lines. Hence the feature maps of the inputed image:

![cn1_fmaps]

consist of slightly shifted and rotated representations of the original image.
